{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 newsgroups dataset shows NMF parts-based decomposition\n",
    "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'sci.space',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'talk.politics.guns', 'sci.space', 'talk.politics.misc']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2461,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2461,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 1, 4, 0, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:10] #  The target attribute is the integer index of the category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vectors from train set using CountVectorizer\n",
    "See http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=0.01,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.9, min_df=0.01)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557.5321444659795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "557.53214446598042"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=10)\n",
    "documented_to_topics_weights = model.fit_transform(X)\n",
    "\n",
    "print(model.reconstruction_err_)\n",
    "\n",
    "def reconsruction_error(X, reconstructed_X):\n",
    "    return np.sqrt(np.power((X - reconstructed_X), 2).sum())\n",
    "    \n",
    "reconsruction_error(\n",
    "    X,\n",
    "    reconstructed_X=documented_to_topics_weights.dot(model.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract components using NMF from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0-rc0\n",
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Embedding, Reshape, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import nonneg\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "K.set_session(sess)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "\n",
    "n_docs, n_words = X.shape\n",
    "K = embedding_size = 10\n",
    "\n",
    "X_doc_ids = np.arange(n_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_contraint = nonneg()\n",
    "docs_to_embeddings = Embedding(input_dim=n_docs, output_dim=embedding_size, input_length=1, \n",
    "                                init='uniform', W_constraint=weights_contraint)\n",
    "\n",
    "embeddings_to_docs = Dense(output_dim=n_words, bias=False, W_constraint=weights_contraint)\n",
    "\n",
    "model = Sequential([\n",
    "    docs_to_embeddings,\n",
    "    Reshape((embedding_size,)), # getting rid of the superfluous dimension of input_length=1\n",
    "    embeddings_to_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "\n",
    "model.fit(\n",
    "    x=X_doc_ids, \n",
    "    y=X.toarray(),\n",
    "    batch_size=32, verbose=0, nb_epoch=500)\n",
    "\n",
    "reconsruction_error(\n",
    "    X,\n",
    "    reconstructed_X=model.predict(X_doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file' 'example' 'argument' 'does' 'used' 'new' 'true' 'god']\n",
      "['law' 'crime' 'firearms' 'guns' 'gun' 'president' '000' 'mr']\n",
      "['going' 'president' 'said' 'know' 'll' 'states' 'mr' 'did']\n",
      "['nasa' 'launch' 'earth' 'data' 'satellite' '10' 'orbit' 'space']\n",
      "['list' 'information' 'launch' 'space' 'nasa' 'file' 'edu' 'mr']\n",
      "['mr' 'president' 'bible' 'god' 'don' 'know' 'does' 'jesus']\n",
      "['people' 'like' 'president' 'just' 'don' 'know' 'mr' 'think']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "for component in model.layers[2].weights[0].eval():\n",
    "    indices = np.argpartition(component, -10)[-8:]\n",
    "    print(feature_names[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
